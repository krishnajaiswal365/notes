To work with the GoEmotions dataset, convert text and emotion classes, and then use BERT tokenization along with RandomForest and XGBoost for classification, you can follow the steps below. This example assumes you have the GoEmotions dataset in a CSV format.

### Step 1: Install Required Libraries

First, ensure you have the necessary libraries installed:

```bash
pip install pandas scikit-learn transformers xgboost
```

### Step 2: Load and Preprocess the Dataset

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from transformers import BertTokenizer

# Load the dataset
df = pd.read_csv('goemotions.csv')  # Replace with your actual file path

# Assuming the dataset has columns 'text' and 'emotion'
texts = df['text'].tolist()
emotions = df['emotion'].tolist()

# Encode emotions to numerical labels
label_encoder = LabelEncoder()
encoded_emotions = label_encoder.fit_transform(emotions)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(texts, encoded_emotions, test_size=0.2, random_state=42)
```

### Step 3: Tokenize Text Using BERT

```python
# Load the BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Tokenize the training and testing texts
train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=128, return_tensors='pt')
test_encodings = tokenizer(X_test, truncation=True, padding=True, max_length=128, return_tensors='pt')
```

### Step 4: Convert Tokenized Data to NumPy Arrays

```python
import numpy as np

# Convert tokenized data to numpy arrays
train_input_ids = train_encodings['input_ids'].numpy()
train_attention_mask = train_encodings['attention_mask'].numpy()

test_input_ids = test_encodings['input_ids'].numpy()
test_attention_mask = test_encodings['attention_mask'].numpy()
```

### Step 5: Train RandomForest and XGBoost Models

```python
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score

# Flatten the input arrays for RandomForest and XGBoost
train_input_ids_flat = train_input_ids.reshape(train_input_ids.shape[0], -1)
test_input_ids_flat = test_input_ids.reshape(test_input_ids.shape[0], -1)

# Train RandomForest
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(train_input_ids_flat, y_train)

# Predict and evaluate RandomForest
rf_predictions = rf_model.predict(test_input_ids_flat)
rf_accuracy = accuracy_score(y_test, rf_predictions)
print(f'RandomForest Accuracy: {rf_accuracy}')

# Train XGBoost
xgb_model = XGBClassifier(n_estimators=100, random_state=42)
xgb_model.fit(train_input_ids_flat, y_train)

# Predict and evaluate XGBoost
xgb_predictions = xgb_model.predict(test_input_ids_flat)
xgb_accuracy = accuracy_score(y_test, xgb_predictions)
print(f'XGBoost Accuracy: {xgb_accuracy}')
```

### Step 6: Evaluate the Models

```python
from sklearn.metrics import classification_report

# Print classification report for RandomForest
print("RandomForest Classification Report:")
print(classification_report(y_test, rf_predictions, target_names=label_encoder.classes_))

# Print classification report for XGBoost
print("XGBoost Classification Report:")
print(classification_report(y_test, xgb_predictions, target_names=label_encoder.classes_))
```

### Notes:
- The BERT tokenizer converts text into token IDs and attention masks, which are then flattened for use with traditional machine learning models like RandomForest and XGBoost.
- The performance of these models might not be as good as using a fine-tuned BERT model directly, but this approach is simpler and faster to implement.
- You can further optimize the models by tuning hyperparameters or using more sophisticated feature extraction techniques.

This code provides a basic pipeline for working with the GoEmotions dataset using BERT tokenization and traditional machine learning models.
